import seaborn as sns
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import streamlit as st
import requests
import pandas as pd
from datetime import datetime, timedelta

column_map = {
    "ä¸­æ–‡": {
        "Title": "èŒä½",
        "Company": "å…¬å¸",
        "Location": "åœ°ç‚¹",
        "Posted On": "å‘å¸ƒæ—¥æœŸ",
        "Deadline": "æˆªæ­¢æ—¶é—´",
        "Skills": "æŠ€èƒ½æ ‡ç­¾",
        "Job Type": "å·¥ä½œç±»å‹",
        "Language": "è¯­è¨€è¦æ±‚"
    },
    "English": {
        "èŒä½": "Title",
        "å…¬å¸": "Company",
        "åœ°ç‚¹": "Location",
        "å‘å¸ƒæ—¥æœŸ": "Posted On",
        "æˆªæ­¢æ—¶é—´": "Deadline",
        "æŠ€èƒ½æ ‡ç­¾": "Skills",
        "å·¥ä½œç±»å‹": "Job Type",
        "è¯­è¨€è¦æ±‚": "Language"
    }
}

# choose your language
language = st.radio("ğŸŒ é€‰æ‹©è¯­è¨€ / Select Language", ["ä¸­æ–‡", "English"], horizontal=True)

# text dictionary
T = {
    "ä¸­æ–‡": {
        "title": "JobLens ç‘å…¸æ‹›è˜ä¿¡æ¯åˆ†æä»ª ğŸ‡¸ğŸ‡ª",
        "desc": "è¾“å…¥å…³é”®è¯ï¼Œè·å–ç‘å…¸æ‹›è˜æ•°æ®ï¼Œå¹¶å®æ—¶å±•ç¤ºç»“æœï¼",
        "input_keyword": "ğŸ” è¾“å…¥å…³é”®è¯ï¼ˆä¾‹å¦‚ï¼šdata scientistï¼‰",
        "slider_label": "ğŸ¯ æœ€å¤šæ˜¾ç¤ºå¤šå°‘æ¡æ‹›è˜ä¿¡æ¯",
        "start_button": "å¼€å§‹æœç´¢",
        "start_searching": "æ­£åœ¨æŠ“å–æ•°æ®ï¼Œè¯·ç¨å€™...",
        "no_result": "æœªæ‰¾åˆ°ç›¸å…³èŒä½ï¼Œè¯·å°è¯•å…¶ä»–å…³é”®è¯ã€‚",
        "API_error": "API è¯·æ±‚å¤±è´¥ï¼Œè¯·ç¨åå†è¯•ã€‚",
        "filter_title": "ğŸ§© æŒ‰æ ‡ç­¾ç­›é€‰å²—ä½",
        "skill_label": "é€‰æ‹©æŠ€èƒ½æ ‡ç­¾ï¼š",
        "jobtype_label": "é€‰æ‹©å·¥ä½œç±»å‹ï¼š",
        "lang_label": "é€‰æ‹©è¯­è¨€è¦æ±‚ï¼š",
        "soon_only": "åªçœ‹å³å°†æˆªæ­¢çš„èŒä½ï¼ˆğŸ”¥3å¤©å†…ï¼‰",
        "result_count": "ğŸ¯ å…±æ‰¾åˆ° {} ä¸ªç¬¦åˆæ¡ä»¶çš„å²—ä½",
        "wordcloud_title": "ğŸ“Š èŒä½åç§°å…³é”®è¯è¯äº‘",
        "city_title": "ğŸ™ï¸ æ‹›è˜èŒä½æœ€å¤šçš„åŸå¸‚åˆ†å¸ƒ",
        "download_button": "ğŸ“¥ ä¸‹è½½å²—ä½æ•°æ®ï¼ˆCSVï¼‰",
        "full_time": "ğŸŸ© å…¨èŒ",
        "part_time": "ğŸŸ§ å…¼èŒ",
        "internship": "ğŸŸ¦ å®ä¹ ",
        "english": "ğŸŸ¨ è‹±è¯­",
        "swedish": "ğŸŸª ç‘å…¸è¯­"
    },
    "English": {
        "title": "JobLens: Swedish Job Analyzer ğŸ‡¸ğŸ‡ª",
        "desc": "Enter a keyword to fetch and explore job opportunities in Sweden!",
        "input_keyword": "ğŸ” Enter keyword (e.g., data scientist)",
        "slider_label": "ğŸ¯ Max number of jobs to fetch",
        "start_button": "Search",
        "start_searching": "Fetching data, please wait...",
        "no_result": "No relevant jobs found. Try another keyword.",
        "API_error": "API request failed, please try again later.",
        "filter_title": "ğŸ§© Filter by Tags",
        "skill_label": "Select skills:",
        "jobtype_label": "Select job types:",
        "lang_label": "Select language requirements:",
        "soon_only": "Only show positions ending soon (ğŸ”¥ within 3 days)",
        "result_count": "ğŸ¯ Found {} matching positions",
        "wordcloud_title": "ğŸ“Š Word Cloud of Job Titles",
        "city_title": "ğŸ™ï¸ Top Cities by Job Count",
        "download_button": "ğŸ“¥ Download as CSV",
        "full_time": "ğŸŸ© Full-Time",
        "part_time": "ğŸŸ§ Part-Time",
        "internship": "ğŸŸ¦ Internship",
        "english": "ğŸŸ¨ English",
        "swedish": "ğŸŸª Swedish"
    }
}

# page title and description
st.title(T[language]["title"])
st.write(T[language]["desc"])

# input of user
keyword = st.text_input(T[language]["input_keyword"], "data scientist")  # åˆ›å»ºæ–‡æœ¬æ¡†
limit = st.slider(T[language]["slider_label"], 10, 100, 20)  # åˆ›å»ºæ»‘åŠ¨æ¡

# --- Automatically load previously saved data when the page loads ---
df = None
if "df" in st.session_state:
    df = st.session_state["df"]

# --- When clicking the button ---
if st.button(T[language]["start_button"]):
    with st.spinner(T[language]["start_searching"]):
        # Constructing the API request address
        url = f"https://jobsearch.api.jobtechdev.se/search?q={keyword}&limit={limit}"
        headers = {"Accept": "application/json"}

        response = requests.get(url, headers=headers)

        if response.status_code == 200:
            data = response.json()

            # Extracting useful fields
            jobs = data.get("hits", [])
            if not jobs:
                st.warning(T[language]["no_result"])
            else:
                job_list = []
                for job in jobs:
                    desc_text = job.get("description", {}).get("text", "").lower()

                    # initial label list
                    skills = []
                    job_type = ""
                    job_language = ""

                    # Skill Matching
                    for skill in ["python", "sql", "excel", "java", "machine learning", "aws", "r", "docker"]:
                        if skill in desc_text:
                            skills.append(skill.upper())

                    # Work type Matchin
                    if "full-time" in desc_text or "heltid" in desc_text:
                        job_type = T[language]["full_time"]
                    elif "part-time" in desc_text or "deltid" in desc_text:
                        job_type = T[language]["part_time"]
                    elif "internship" in desc_text or "praktik" in desc_text:
                        job_type = T[language]["internship"]

                    # Job language requirement
                    if "english" in desc_text:
                        job_language = T[language]["english"]
                    elif "swedish" in desc_text or "svenska" in desc_text:
                        job_language = T[language]["swedish"]

                    # get publication and application list
                    published_raw = job.get("publication_date")
                    deadline_raw = job.get("application_deadline")

                    # initial string
                    published = ""
                    deadline = ""

                    # publication timeï¼šonly date
                    if published_raw:
                        try:
                            dt_pub = datetime.fromisoformat(published_raw)
                            published = dt_pub.strftime("%Y-%m-%d")
                        except:
                            published = ""

                    # deadlineï¼šdate and time
                    if deadline_raw:
                        try:
                            dt_deadline = datetime.fromisoformat(deadline_raw)
                            deadline = dt_deadline.strftime("%Y-%m-%d %H:%M")
                        except:
                            deadline = ""

                    # ç»„ç»‡ä¿¡æ¯
                    job_info = {
                        "èŒä½": job.get("headline"),
                        "å…¬å¸": job.get("employer", {}).get("name"),
                        "åœ°ç‚¹": job.get("workplace_address", {}).get("municipality"),
                        "å‘å¸ƒæ—¥æœŸ": published,
                        "æˆªæ­¢æ—¶é—´": deadline,
                        "é“¾æ¥": job.get("webpage_url"),
                        "æŠ€èƒ½æ ‡ç­¾": ", ".join(skills),
                        "å·¥ä½œç±»å‹": job_type,
                        "è¯­è¨€è¦æ±‚": job_language
                    }

                    job_list.append(job_info)

                # transfer toDataFrame
                df = pd.DataFrame(job_list)
                st.session_state["df"] = df  # save the data

                # æ˜¾ç¤ºè¡¨æ ¼
                #st.success(f"å…±æ‰¾åˆ° {len(df)} ä¸ªèŒä½ï¼š")
                #st.dataframe(df)
        else:
            st.error(T[language]["API_error"])

if df is not None and not df.empty:
    # Filter jobs by tag
    st.subheader(T[language]["filter_title"])

    skill_options = sorted(set(skill for s in df["æŠ€èƒ½æ ‡ç­¾"] for skill in s.split(", ") if s))
    selected_skills = st.multiselect(T[language]["skill_label"], skill_options)

    job_type_options = df["å·¥ä½œç±»å‹"].dropna().unique().tolist()
    selected_job_types = st.multiselect(T[language]["jobtype_label"], job_type_options)

    language_options = df["è¯­è¨€è¦æ±‚"].dropna().unique().tolist()
    selected_languages = st.multiselect(T[language]["lang_label"], language_options)

    filtered_df = df.copy()

    if selected_skills:
        filtered_df = filtered_df[filtered_df["æŠ€èƒ½æ ‡ç­¾"].apply(
            lambda tags: all(skill in tags.split(", ") for skill in selected_skills))]

    if selected_job_types:
        filtered_df = filtered_df[filtered_df["å·¥ä½œç±»å‹"].isin(selected_job_types)]

    if selected_languages:
        filtered_df = filtered_df[filtered_df["è¯­è¨€è¦æ±‚"].isin(selected_languages)]

    if st.checkbox(T[language]["soon_only"]):
        filtered_df = filtered_df[filtered_df["æˆªæ­¢æ—¶é—´"].str.contains("ğŸ”¥")]


    filtered_df = filtered_df.sort_values(by="å‘å¸ƒæ—¥æœŸ", ascending=False)

    st.write(T[language]["result_count"].format(len(filtered_df)))

    # Convert the job title column into a markdown hyperlink with a link
    filtered_df["èŒä½"] = filtered_df.apply(
        lambda row: f"[{row['èŒä½']}]({row['é“¾æ¥']})" if pd.notna(row["é“¾æ¥"]) else row["èŒä½"],
        axis=1
    )

    # Remove the original "Link" column
    filtered_df.drop(columns=["é“¾æ¥"], inplace=True)

    # change column name according to web language
    filtered_df.rename(columns=column_map[language], inplace=True)

    # show the column
    st.markdown(filtered_df.to_markdown(index=False), unsafe_allow_html=True)

    # CSV ä¸‹è½½æŒ‰é’®
    csv = filtered_df.to_csv(index=False).encode("utf-8")
    st.download_button(T[language]["download_button"], csv, file_name="jobs.csv", mime="text/csv")

    # Word Cloud
    if not df.empty:
        st.subheader(T[language]["wordcloud_title"])

        # concat the job title
        text = " ".join(df["èŒä½"].dropna().astype(str))

        stopwords = set(["for", "and", "to", "with", "of", "in", "the", "a", "on", "as"])
        wordcloud = WordCloud(width=800, height=400, background_color="white", stopwords=stopwords).generate(text)

        fig, ax = plt.subplots()
        ax.imshow(wordcloud, interpolation="bilinear")
        ax.axis("off")
        st.pyplot(fig)

    # City distribution map
    if not df.empty:
        st.subheader(T[language]["city_title"])

        city_counts = df["åœ°ç‚¹"].value_counts().head(10)
        fig, ax = plt.subplots()
        sns.barplot(x=city_counts.values, y=city_counts.index, ax=ax)
        ax.set_xlabel("Number of Positions")
        ax.set_ylabel("City")
        st.pyplot(fig)